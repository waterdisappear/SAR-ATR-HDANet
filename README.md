<h1 align="center"> HDANet: Hierarchical Disentanglement-Alignment Network for Robust SAR Vehicle Recognition </h1> 

<h5 align="center"><em> Weijie Li (æç®æ°), Wei Yang (æ¨å¨), Wenpeng Zhang (å¼ æ–‡é¹), Tianpeng Liu (åˆ˜å¤©é¹), Yongxiang (åˆ˜æ°¸ç¥¥), and Li Liu (åˆ˜ä¸½) </em></h5>

<p align="center">
  <a href="#Introduction">Introduction</a> |
  <a href="#Data">Data</a> |
  <a href="#HDANet">HDANet</a> |
  <a href="#Statement">Statement</a>
</p >

<p align="center">
<a href="https://ieeexplore.ieee.org/document/10283916"><img src="https://img.shields.io/badge/Paper-IEEE%20J--STARS-blue"></a>
<a href="https://arxiv.org/abs/2304.03550"><img src="https://img.shields.io/badge/Paper-arxiv-red"></a>
<a href="https://zhuanlan.zhihu.com/p/787306380"><img src="https://img.shields.io/badge/æ–‡ç« -çŸ¥ä¹-blue"></a>  
</p>

## Introduction

This paper proposes a novel domain alignment framework, Hierarchical Disentanglement-Alignment Network (HDANet), to enhance SAR ATR features' causality and robustness. If you find our work is useful, please give us a star ğŸŒŸ in GitHub and cite our paper in the BibTex format at the end.

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸŸå¯¹é½æ¡†æ¶ï¼Œåˆ†å±‚è§£è€¦å¯¹é½ç½‘ç»œï¼ˆHDANetï¼‰ï¼Œä»¥å¢å¼ºSARç›®æ ‡è¯†åˆ«ç‰¹å¾çš„å› æœæ€§å’Œé²æ£’æ€§ã€‚å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰ä»·å€¼ï¼Œè¯·åœ¨ GitHub ä¸Šç»™æˆ‘ä»¬ä¸ªæ˜Ÿæ˜Ÿ ğŸŒŸ å¹¶æŒ‰é¡µé¢æœ€åçš„ BibTex æ ¼å¼å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚

<figure>
<div align="center">
<img src=example/fig_framework.png width="90%">
</div>
</figure>

**Abstract:** Vehicle recognition is a fundamentale problem in synthetic aperture radar (SAR) image interpretation. However, robustly recognizing vehicle targets is a challenging task in SAR due to the large intraclass variations and small interclass variations. In addition, the lack of large datasets further complicates the task. Inspired by the analysis of target signature variations and deep learning explainability, this article proposes a novel domain alignment framework, named the hierarchical disentanglement-alignment network (HDANet), to achieve robustness under various operating conditions. Concisely, HDANet integrates feature disentanglement and alignment into a unified framework with three modules: domain data generation; multitask-assisted mask disentanglement; and the domain alignment of target features. The first module generates diverse data for alignment, and three simple but effective data augmentation methods are designed to simulate target signature variations. The second module disentangles the target features from background clutter using the multitask-assisted mask to prevent clutter from interfering with subsequent alignment. The third module employs a contrastive loss for domain alignment to extract robust target features from generated diverse data and disentangled features. Finally, the proposed method demonstrates impressive robustness across nine operating conditions in the MSTAR dataset, and extensive qualitative and quantitative analyses validate the effectiveness of our framework. 

**æ‘˜è¦:** è½¦è¾†è¯†åˆ«æ˜¯åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒè§£è¯‘ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºç±»å†…å·®å¼‚å¤§ã€ç±»é—´å·®å¼‚å°ï¼Œåœ¨SARä¸­é²æ£’æ€§åœ°è¯†åˆ«è½¦è¾†ç›®æ ‡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¤§å‹æ•°æ®é›†ä¹Ÿä½¿è¿™é¡¹ä»»åŠ¡å˜å¾—æ›´åŠ å¤æ‚ã€‚å—ç›®æ ‡ç‰¹å¾å˜åŒ–åˆ†æå’Œæ·±åº¦å­¦ä¹ å¯è§£é‡Šæ€§çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸŸå¯¹é½æ¡†æ¶ï¼Œå‘½åä¸ºåˆ†å±‚è§£è€¦å¯¹é½ç½‘ç»œï¼ˆHDANetï¼‰ï¼Œä»¥å®ç°å„ç§æ“ä½œæ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒHDANet å°†ç‰¹å¾è§£è€¦å’Œå¯¹é½é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šåŸŸæ•°æ®ç”Ÿæˆã€å¤šä»»åŠ¡è¾…åŠ©æ©ç è§£è€¦å’Œç›®æ ‡ç‰¹å¾çš„åŸŸå¯¹é½ã€‚ç¬¬ä¸€ä¸ªæ¨¡å—ç”Ÿæˆç”¨äºåŸŸå¯¹é½çš„å„ç§åŸŸæ•°æ®ï¼Œè®¾è®¡äº†ä¸‰ç§ç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•æ¥æ¨¡æ‹Ÿç›®æ ‡ç‰¹å¾çš„å˜åŒ–ã€‚ç¬¬äºŒä¸ªæ¨¡å—ä½¿ç”¨å¤šä»»åŠ¡è¾…åŠ©æ©ç å°†ç›®æ ‡ç‰¹å¾ä»èƒŒæ™¯æ‚æ³¢ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œä»¥é˜²æ­¢æ‚æ³¢å¹²æ‰°åç»­å¯¹é½ã€‚ç¬¬ä¸‰ä¸ªæ¨¡å—é‡‡ç”¨å¯¹æ¯”æŸå¤±è¿›è¡ŒåŸŸå¯¹é½ï¼Œä»ç”Ÿæˆçš„å¤šæ ·åŒ–æ•°æ®å’Œè§£è€¦ç‰¹å¾ä¸­æå–ç¨³å¥çš„ç›®æ ‡ç‰¹å¾ã€‚æœ€åï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ MSTAR æ•°æ®é›†ä¸­çš„ä¹ç§æ“ä½œæ¡ä»¶ä¸‹è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é²æ£’æ€§ï¼Œå¤§é‡çš„å®šæ€§å’Œå®šé‡åˆ†æéªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚

<figure>
<div align="center">
<img src=example/fig_radarmap.png width="60%">
</div>
</figure>

## Data
The folder includes MSTAR images under SOC and EOCs and detailed information can be found in our paper. 

è¯¥æ–‡ä»¶å¤¹åŒ…æ‹¬ SOC å’Œ EOC ä¸‹çš„ MSTAR å›¾åƒï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚é˜…æˆ‘ä»¬çš„è®ºæ–‡ã€‚

## HDANet
Requirements
- Python 
- PyTorch 
- Numpy
- [Captum](https://captum.ai/) (We used captum to generate pseudo-labels.)

A simple demo.

ä¸€ä¸ªç®€å•çš„demo.

```python
from HDANet.utils.DataLoad import load_data, load_test
from HDANet.utils.TrainTest import model_train, model_test
from HDANet.Model.HDANet import HDANet

train_all, label_name = load_data(arg.data_path + 'TRAIN', id=arg.GPU_ids)
test_set, _ = load_test(arg.data_path + 'TEST')
train_loader = torch.utils.data.DataLoader(train_all, batch_size=arg.batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=arg.batch_size, shuffle=False)

model = HDANet(num_classes=len(label_name))
opt = torch.optim.NAdam(model.parameters(), lr=arg.lr, weight_decay=5e-4)
scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.98)
best_test_accuracy = 0

for epoch in range(1, arg.epochs + 1):
    # print("##### " + str(k + 1) + " EPOCH " + str(epoch) + "#####")
    model_train(model=model, data_loader=train_loader, opt=opt)
    scheduler.step()

acc = model_test(model, test_loader)
```

## Statement

- This project is released under the [CC BY-NC 4.0](LICENSE).
- Any questions please contact us at lwj2150508321@sina.com. 
- If you find our work is useful, please give us ğŸŒŸ in GitHub and cite our paper in the following BibTex format:

```
@ARTICLE{li2023hierarchical,
  author={Li, Weijie and Yang, Wei and Zhang, Wenpeng and Liu, Tianpeng and Liu, Yongxiang and Liu, Li},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
  title={Hierarchical Disentanglement-Alignment Network for Robust SAR Vehicle Recognition}, 
  year={2023},
  volume={16},
  number={},
  pages={9661-9679},
  doi={10.1109/JSTARS.2023.3324182}
}
```
